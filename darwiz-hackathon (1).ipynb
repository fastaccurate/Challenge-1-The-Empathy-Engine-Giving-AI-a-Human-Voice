{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12970100,"sourceType":"datasetVersion","datasetId":8208942}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"c4a385d9-f64c-4fb3-b6e5-2aced53cbf31","cell_type":"markdown","source":"# 🎭 The Empathy Engine - End-to-End Emotional AI Voice - Jupyter Notebook\n\n**Transform text into emotionally expressive speech with interactive UI and downloadable audio!**\n\n> *Features included:*\n> - Multi-modal emotion analysis (Hugging Face + VADER)\n> - Dynamic voice modulation (rate, pitch, volume, breathing)\n> - 3D emotion space visualization\n> - Responsive UI (Gradio) with MP3 audio output and quick examples\n> - Single-file implementation: just run and deploy!\n","metadata":{}},{"id":"1fb0a52c-728e-430a-b1fc-a73ae65e8664","cell_type":"code","source":"# Cell 1: Install dependencies (uncomment for Colab)\n!pip install -q gradio transformers torch vaderSentiment pyttsx3 gtts soundfile pydub\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T12:36:58.230299Z","iopub.execute_input":"2025-09-05T12:36:58.230975Z","iopub.status.idle":"2025-09-05T12:38:24.469378Z","shell.execute_reply.started":"2025-09-05T12:36:58.230952Z","shell.execute_reply":"2025-09-05T12:38:24.468410Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":4},{"id":"565f0a4d-20c0-440f-b896-a4394b7a4065","cell_type":"code","source":"# Cell 2: Imports\nimport gradio as gr\nimport torch\nfrom transformers import pipeline\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nimport pyttsx3\nfrom gtts import gTTS\nimport tempfile, os, time\nfrom pydub import AudioSegment\nimport numpy as np\nimport plotly.graph_objects as go\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n# Check if TTS engine is available\nprint(\"🎭 Empathy Engine - Libraries loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T12:38:49.279128Z","iopub.execute_input":"2025-09-05T12:38:49.279866Z","iopub.status.idle":"2025-09-05T12:38:49.317264Z","shell.execute_reply.started":"2025-09-05T12:38:49.279832Z","shell.execute_reply":"2025-09-05T12:38:49.316728Z"}},"outputs":[{"name":"stdout","text":"🎭 Empathy Engine - Libraries loaded successfully!\n","output_type":"stream"}],"execution_count":5},{"id":"f1a01753-dbc3-4f63-9233-2a3babce5d4a","cell_type":"code","source":"# Cell 3: Emotion Analyzer Class\nclass EmotionAnalyzer:\n    def __init__(self):\n        self.emotion_classifier = pipeline(\n            'text-classification',\n            model='j-hartmann/emotion-english-distilroberta-base',\n            device=0 if torch.cuda.is_available() else -1\n        )\n        self.vader = SentimentIntensityAnalyzer()\n        self.emotion_3d_map = {\n            'joy': {'valence': 0.8, 'arousal': 0.7, 'dominance': 0.6},\n            'sadness': {'valence': -0.7, 'arousal': -0.4, 'dominance': -0.5},\n            'anger': {'valence': -0.6, 'arousal': 0.8, 'dominance': 0.7},\n            'fear': {'valence': -0.8, 'arousal': 0.6, 'dominance': -0.8},\n            'surprise': {'valence': 0.2, 'arousal': 0.8, 'dominance': 0.1},\n            'disgust': {'valence': -0.7, 'arousal': 0.3, 'dominance': 0.2},\n            'love': {'valence': 0.9, 'arousal': 0.5, 'dominance': 0.3},\n            'neutral': {'valence': 0.0, 'arousal': 0.0, 'dominance': 0.0}\n        }\n    def analyze(self, text):\n        emotion_out = self.emotion_classifier(text)[0]\n        emo = emotion_out['label'].lower()\n        conf = emotion_out['score']\n        vader_scores = self.vader.polarity_scores(text)\n        intensity = abs(vader_scores['compound'])\n        emo3d = self.emotion_3d_map.get(emo, self.emotion_3d_map['neutral'])\n        scaled_emo3d = {k: v * intensity for k, v in emo3d.items()}\n        return {'emotion': emo, 'confidence': conf, 'emotion_3d': scaled_emo3d, 'intensity': intensity, 'vader': vader_scores}\nemotion_analyzer = EmotionAnalyzer()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T12:38:52.396829Z","iopub.execute_input":"2025-09-05T12:38:52.397106Z","iopub.status.idle":"2025-09-05T12:38:59.223741Z","shell.execute_reply.started":"2025-09-05T12:38:52.397085Z","shell.execute_reply":"2025-09-05T12:38:59.220955Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c98cdeacfc434edb828180103de2f9a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/329M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ca7dfb871a14ab6bf824013e9c14aef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/294 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"502f071b153a457081dfdcfec1d09130"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/329M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08fe9ac6cb0c4f0cab1ba4af0ac6b01c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dfa416662724495a259f0fd6030d228"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e34ae429900e432cb859a86982889370"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01401cc53f4c4a2082d2e810f61a2758"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"076a820b0e94417089c059b6fe47c734"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":6},{"id":"4671a2de-a208-4f6f-80be-96d21f114726","cell_type":"code","source":"# Cell 4: VoiceParameterCalculator\nclass VoiceParameterCalculator:\n    def __init__(self):\n        self.base_rate = 200        # words per minute\n        self.base_pitch = 0.5       # 0–1 scale\n        self.base_volume = 0.8      # 0–1 scale\n\n    def calculate(self, emotion_3d, emotion_name):\n        val = emotion_3d['valence']\n        aro = emotion_3d['arousal']\n        dom = emotion_3d['dominance']\n        # Rate: scale by arousal\n        rate = int(self.base_rate * (1 + aro * 0.5))\n        rate = min(max(rate, 50), 400)\n        # Pitch: combine valence & arousal\n        pitch = self.base_pitch + val * 0.3 + aro * 0.2\n        pitch = min(max(pitch, 0.1), 0.9)\n        # Volume: combine dominance & arousal\n        volume = self.base_volume + dom * 0.2 + aro * 0.15\n        volume = min(max(volume, 0.1), 1.0)\n        # Breathing frequency & pause length\n        breath = 0.1 + aro * 0.05 + (abs(val)*0.02 if val<0 else 0)\n        pause = max(0.1, 0.5 - aro*0.3 - (abs(dom)*0.2 if dom<0 else 0))\n        # Tremor for fear/sadness\n        tremor = min(0.4, aro * abs(val)) if emotion_name in ['fear','sadness'] and val<0 else 0\n        return {\n            'rate': rate,\n            'pitch': pitch,\n            'volume': volume,\n            'breath_freq': breath,\n            'pause_len': pause,\n            'tremor': tremor\n        }\n\nvoice_calculator = VoiceParameterCalculator()\nprint(\"🎛️ VoiceParameterCalculator ready\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T12:41:18.933904Z","iopub.execute_input":"2025-09-05T12:41:18.934568Z","iopub.status.idle":"2025-09-05T12:41:18.941556Z","shell.execute_reply.started":"2025-09-05T12:41:18.934542Z","shell.execute_reply":"2025-09-05T12:41:18.940781Z"}},"outputs":[{"name":"stdout","text":"🎛️ VoiceParameterCalculator ready\n","output_type":"stream"}],"execution_count":7},{"id":"d1ae2da5-8e07-4d3d-a6b3-ebd9b83a6589","cell_type":"code","source":"# Cell 5: AdvancedTTSEngine (gTTS only)\nclass AdvancedTTSEngine:\n    def __init__(self):\n        from pydub import AudioSegment  # for future use if needed\n        print(\"🔊 AdvancedTTSEngine initialized (gTTS)\")\n\n    def _apply_effects(self, text, params):\n        # Insert simple pauses to simulate breathing/emotion\n        words = text.split()\n        out, cnt = [], 0\n        breath_every = max(1, int(1/params['breath_freq']))\n        for w in words:\n            out.append(w)\n            cnt += 1\n            if cnt >= breath_every and params['breath_freq'] > 0:\n                out.append(\"....\")  # short pause\n                cnt = 0\n            if w.endswith(('.', '!', '?')) and params['pause_len'] > 0.6:\n                out.append(\".........\")  # longer pause\n        return \" \".join(out)\n\n    def synthesize(self, text, params, emo_name):\n        # 1) Apply text effects\n        proc_text = self._apply_effects(text, params)\n\n        # 2) Generate MP3 with gTTS\n        tts = gTTS(proc_text, lang=\"en\", slow=False)\n        tmp_mp3 = tempfile.NamedTemporaryFile(suffix=\".mp3\", delete=False).name\n        tts.save(tmp_mp3)\n\n        return tmp_mp3\n\n# Re-initialize TTS engine\ntts_engine = AdvancedTTSEngine()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T13:07:39.592663Z","iopub.execute_input":"2025-09-05T13:07:39.593385Z","iopub.status.idle":"2025-09-05T13:07:39.600284Z","shell.execute_reply.started":"2025-09-05T13:07:39.593362Z","shell.execute_reply":"2025-09-05T13:07:39.599372Z"}},"outputs":[{"name":"stdout","text":"🔊 AdvancedTTSEngine initialized (gTTS)\n","output_type":"stream"}],"execution_count":27},{"id":"be2d2007-bb19-4cf2-9235-3fe2171d40bb","cell_type":"markdown","source":"## SECOND VERSION:","metadata":{}},{"id":"5ac5b136-af06-44c3-a825-aaae19fa1ee8","cell_type":"code","source":"# Cell 5: AdvancedTTSEngine (gTTS with audible breathing and 10% slower speech)\n\nimport os\nimport tempfile\nfrom gtts import gTTS\nfrom pydub import AudioSegment, effects\n\nBREATH_SOUND_PATH = \"/kaggle/input/voicceee/Breathvpice-1.wav\"\nif not os.path.exists(BREATH_SOUND_PATH):\n    raise FileNotFoundError(f\"Breath sound file not found: {BREATH_SOUND_PATH}\")\nbreath_sound = AudioSegment.from_file(BREATH_SOUND_PATH)\n\nclass AdvancedTTSEngine:\n    def __init__(self):\n        print(\"🔊 AdvancedTTSEngine initialized (gTTS, audible breathing, 10% slower)\")\n\n    def synthesize(self, text, params, emo_name):\n        words = text.split()\n        chunks, wcnt, curr = [], 0, []\n        base_interval = max(1, int(1 / params[\"breath_freq\"]))\n        breath_interval = base_interval * 2  # breaths twice less frequent than base frequency\n\n        for w in words:\n            curr.append(w)\n            wcnt += 1\n            if wcnt >= breath_interval:\n                chunks.append((\" \".join(curr), \"BREATH\"))\n                curr, wcnt = [], 0\n            if w.endswith((\".\", \"!\", \"?\")):\n                chunks.append((\" \".join(curr), \"PAUSE\"))\n                curr, wcnt = [], 0\n        if curr:\n            chunks.append((\" \".join(curr), None))\n\n        final_audio = AudioSegment.empty()\n        intensity = params.get(\"tremor\", 0) + params.get(\"breath_freq\", 0)\n\n        for segment_text, marker in chunks:\n            # Generate speech chunk using gTTS\n            tts = gTTS(segment_text, lang=\"en\", slow=False)\n            tmp_file = tempfile.NamedTemporaryFile(suffix=\".mp3\", delete=False).name\n            tts.save(tmp_file)\n            seg_audio = AudioSegment.from_file(tmp_file, format=\"mp3\")\n\n            # Amplify stressed segments (with '!' or '?') when intensity high\n            if intensity > 0.8 and (\"!\" in segment_text or \"?\" in segment_text):\n                seg_audio += 50\n\n            final_audio += seg_audio\n\n            # Insert pauses or breaths\n            if marker == \"PAUSE\":\n                final_audio += AudioSegment.silent(duration=600)  # 0.6 seconds pause\n            elif marker == \"BREATH\":\n                final_audio += breath_sound + 2  # mildly louder breathing sound\n\n        # Slow down entire audio to 90% speed (playback duration ~1.11x longer)\n        final_audio = final_audio._spawn(final_audio.raw_data, overrides={\n            \"frame_rate\": int(final_audio.frame_rate * 0.9)\n        }).set_frame_rate(final_audio.frame_rate)\n\n        # Normalize and compress\n        final_audio = effects.normalize(final_audio)\n        final_audio = effects.compress_dynamic_range(final_audio, threshold=-20.0, ratio=2.5)\n\n        # Export enhanced MP3\n        out_path = tempfile.NamedTemporaryFile(suffix=\"_enhanced.mp3\", delete=False).name\n        final_audio.export(out_path, format=\"mp3\")\n\n        return out_path\n\n# Reinitialize TTS engine\ntts_engine = AdvancedTTSEngine()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T14:06:53.556853Z","iopub.execute_input":"2025-09-05T14:06:53.557378Z","iopub.status.idle":"2025-09-05T14:06:53.570757Z","shell.execute_reply.started":"2025-09-05T14:06:53.557330Z","shell.execute_reply":"2025-09-05T14:06:53.570035Z"}},"outputs":[{"name":"stdout","text":"🔊 AdvancedTTSEngine initialized (gTTS, audible breathing, 10% slower)\n","output_type":"stream"}],"execution_count":67},{"id":"8fb3856d-e6a8-40c3-8ccf-39df06522cf5","cell_type":"code","source":"# Cell 6: Visualization functions\ndef plot_3d(emotion_3d, emo, conf):\n    fig = go.Figure()\n    fig.add_trace(go.Scatter3d(\n        x=[emotion_3d['valence']], y=[emotion_3d['arousal']],\n        z=[emotion_3d['dominance']], mode='markers+text',\n        marker=dict(size=12, color=conf, colorscale='Viridis', showscale=True),\n        text=[f\"{emo.title()}<br>{conf:.0%}\"], textposition=\"top center\"\n    ))\n    # reference points\n    ref = emotion_analyzer.emotion_3d_map\n    xs, ys, zs, names = [],[],[],[]\n    for e,(v,a,d) in ref.items():\n        xs.append(v); ys.append(a); zs.append(d); names.append(e)\n    fig.add_trace(go.Scatter3d(\n        x=xs, y=ys, z=zs, mode='markers+text',\n        marker=dict(size=6, color='lightgray', opacity=0.6),\n        text=names, textposition=\"bottom right\", name=\"refs\"\n    ))\n    fig.update_layout(\n        scene=dict(\n            xaxis_title=\"Valence\", yaxis_title=\"Arousal\", zaxis_title=\"Dominance\"\n        ), width=600, height=500, title=\"3D Emotion Space\"\n    )\n    return fig\n\ndef plot_radar(emotion_3d, emo):\n    vals = [emotion_3d[k] for k in ['valence','arousal','dominance']]\n    norm = [(v+1)/2 for v in vals]\n    cat = ['Valence','Arousal','Dominance']\n    fig = go.Figure(go.Scatterpolar(\n        r=norm+[norm[0]], theta=cat+[cat[0]], fill='toself',\n        name=emo.title(), line=dict(color='deeppink')\n    ))\n    fig.update_layout(polar=dict(radialaxis=dict(range=[0,1])), title=\"Emotion Radar\", width=400, height=400)\n    return fig\n\nprint(\"📊 Visualization ready\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T13:35:18.467201Z","iopub.execute_input":"2025-09-05T13:35:18.467484Z","iopub.status.idle":"2025-09-05T13:35:18.476259Z","shell.execute_reply.started":"2025-09-05T13:35:18.467464Z","shell.execute_reply":"2025-09-05T13:35:18.475586Z"}},"outputs":[{"name":"stdout","text":"📊 Visualization ready\n","output_type":"stream"}],"execution_count":41},{"id":"64f92682-5907-4a9f-993a-4e59b4d4be1a","cell_type":"code","source":"# Cell 7: process(text) → audio, markdown, plots, DataFrame\ndef process(text):\n    if not text.strip():\n        return None, \"Please enter some text.\", None, None, pd.DataFrame()\n    # Emotion analysis\n    e = emotion_analyzer.analyze(text)\n    emo, conf, emo3d = e['emotion'], e['confidence'], e['emotion_3d']\n    # Voice params\n    vp = voice_calculator.calculate(emo3d, emo)\n    # Audio MP3\n    audio_path = tts_engine.synthesize(text, vp, emo)\n    # Visuals\n    fig3d = plot_3d(emo3d, emo, conf)\n    figr = plot_radar(emo3d, emo)\n    # Summary markdown\n    md = f\"\"\"\n**Emotion:** {emo.title()} ({conf:.1%})  \n**Rate:** {vp['rate']} WPM • **Pitch:** {vp['pitch']:.1%} • **Volume:** {vp['volume']:.1%}  \n\"\"\"\n    # Table\n    df = pd.DataFrame([\n        ['Valence', f\"{emo3d['valence']:.3f}\"],\n        ['Arousal', f\"{emo3d['arousal']:.3f}\"],\n        ['Dominance', f\"{emo3d['dominance']:.3f}\"],\n        ['Intensity', f\"{e['intensity']:.3f}\"]\n    ], columns=['Metric','Value'])\n    return audio_path, md, fig3d, figr, df\n\nprint(\"⚙️ Main pipeline ready\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T13:35:20.258125Z","iopub.execute_input":"2025-09-05T13:35:20.258813Z","iopub.status.idle":"2025-09-05T13:35:20.264638Z","shell.execute_reply.started":"2025-09-05T13:35:20.258789Z","shell.execute_reply":"2025-09-05T13:35:20.264010Z"}},"outputs":[{"name":"stdout","text":"⚙️ Main pipeline ready\n","output_type":"stream"}],"execution_count":42},{"id":"dc2a0bad-99c2-4c70-b68e-d73e31f6eedf","cell_type":"code","source":"# Cell 8: Gradio interface (fixed Audio component)\nwith gr.Blocks(title=\"🎭 The Empathy Engine\") as demo:\n    gr.Markdown(\"## 🎭 Empathy Engine: Emotional TTS\")\n    with gr.Row():\n        txt = gr.Textbox(lines=4, placeholder=\"Type your text here...\")\n        btn = gr.Button(\"Generate Speech\")\n    out_audio = gr.Audio(type=\"filepath\", label=\"Emotional Speech (MP3)\")\n    out_md    = gr.Markdown()\n    with gr.Row():\n        plot3d = gr.Plot(label=\"3D Emotion Space\")\n        radar   = gr.Plot(label=\"Emotion Radar\")\n    table    = gr.DataFrame(headers=[\"Metric\",\"Value\"], label=\"Emotion Details\")\n    btn.click(fn=process, inputs=txt,\n              outputs=[out_audio, out_md, plot3d, radar, table])\ndemo.launch(share=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T14:06:57.704208Z","iopub.execute_input":"2025-09-05T14:06:57.704839Z","iopub.status.idle":"2025-09-05T14:06:59.247120Z","shell.execute_reply.started":"2025-09-05T14:06:57.704815Z","shell.execute_reply":"2025-09-05T14:06:59.246493Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7875\n* Running on public URL: https://0903f257802e2d7960.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://0903f257802e2d7960.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":68,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":68},{"id":"260daa0b-b379-4c4f-a94b-4e89ab28a34d","cell_type":"code","source":"“Ugh, I’m stuck in traffic again and it’s driving me crazy!”\n\n“I just got promoted at work—I’m over the moon about this!”\n\n“I can’t believe they canceled the event last minute; I’m really disappointed.”\n\n“This new opportunity feels both thrilling and a bit nerve-wracking.”\n\n“I’m so proud of you for everything you’ve accomplished.”\n\n\n\n“The movie’s plot twist left me speechless and amazed.”\n\n“I’m worried about tomorrow’s presentation; what if I mess up?”\n\n“That joke was hilarious—I haven’t laughed that hard in ages!”\n\n“I feel a deep sense of loss after saying goodbye today.”\n\n“Wow, that surprise party was the best birthday gift ever!”","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b4bbe106-5c1f-4c4c-adf3-79407442cd62","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}